{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re,datetime\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Completing the In class Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is a programming model for performing parallel processing on Big Data. It is powerful, yet relatively simple.\n",
    "\n",
    "There are two basic steps:\n",
    "1. _Mapper_ - Turn each item in zero or more key-value pairs.\n",
    "2. _Reducer_ - Produce output values by grouping together values from each corresponding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'data': 5, 'science': 1, 'big': 1, 'problems': 1})\n"
     ]
    }
   ],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return (all_words)                           # remove duplicates\n",
    "\n",
    "# Old way of counting words\n",
    "def word_count_old(documents):\n",
    "    \"\"\"word count not using MapReudce\"\"\"\n",
    "    return Counter(word\n",
    "                  for document in documents\n",
    "                  for word in tokenize(document))\n",
    "\n",
    "documents = [\"data data data science\",\"big data\",\"data problems\"]\n",
    "result = word_count_old(documents)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The mapper functions maps the task\n",
    "def wc_mapper(document):\n",
    "    \"\"\"for each word in document, emit (word,1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word,1)\n",
    "        \n",
    "# The reducer function collects the results\n",
    "def wc_reducer(word, counts):\n",
    "    \"\"\"sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))\n",
    "    \n",
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "    \n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "            \n",
    "    # add a statement to print the collector here\n",
    "    print(collector)        \n",
    "    return [output\n",
    "            # replace items() with iteritems() if you get an error\n",
    "           for word, counts in collector.items()\n",
    "           for output in wc_reducer(word,counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of documents where there is some overlap in the words in each document (don't use more than a total of about 5-6 words). Use your word_count function on this list.\n",
    "\n",
    "e.g., [\"data science\", \"big data\", \"science fiction\", \"data mining\"]\n",
    "\n",
    "Add a print statment to the function so you can see the values in collector after the mapper function has run.\n",
    "\n",
    "What if a document has more than one occurence of a word? e.g., \"data data science\" Can you alter the tokenize function to fix this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'data': [1, 1, 1], 'science': [1, 1], 'big': [1], 'fiction': [1], 'mining': [1]})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('data', 3), ('science', 2), ('big', 1), ('fiction', 1), ('mining', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of documents here\n",
    "documents = [\"data science\", \"big data\", \"science fiction\", \"data mining\"]\n",
    "word_count(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 3), ('science', 2), ('big', 1), ('fiction', 1), ('mining', 1)]\n"
     ]
    }
   ],
   "source": [
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"runs MapReduce on input using functions mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    # write a for loop over the inputs that calls mapper\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "            \n",
    "    # add a statement to print the collector here\n",
    "    #print(collector)        \n",
    "    \n",
    "    # write a return statement that calls the reducer\n",
    "    return [output\n",
    "            # replace items() with iteritems() if you get an error\n",
    "           for input, values in collector.items()\n",
    "           for output in reducer(input,values)]\n",
    "word_counts = map_reduce(documents, wc_mapper, wc_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_values_using(aggregation_fn, key, values):\n",
    "    \"\"\"reduces a key-values pair by applying aggregation_fn\"\"\"\n",
    "    yield (key, aggregation_fn(values))\n",
    "    \n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\"turns a functions (values->output) into a reducer that\n",
    "    maps (key, values)->(key, output)\"\"\"\n",
    "    return partial(reduce_values_using, aggregation_fn)\n",
    "\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 3), ('science', 2), ('big', 1), ('fiction', 1), ('mining', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, sum_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 1), ('science', 1), ('big', 1), ('fiction', 1), ('mining', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, max_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 1), ('science', 1), ('big', 1), ('fiction', 1), ('mining', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, min_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 1), ('science', 1), ('big', 1), ('fiction', 1), ('mining', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, count_distinct_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_updates = [\n",
    "    {\"id\": 1, \n",
    "     \"username\" : \"joelgrus\", \n",
    "     \"text\" : \"Is anyone interested in a data science book?\",\n",
    "     \"created_at\" : datetime.datetime(2013, 12, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"data_guy\", \"data_gal\", \"bill\"] },\n",
    "    # add your own\n",
    "    {\"id\": 2, \n",
    "     \"username\" : \"dnisarg13\", \n",
    "     \"text\" : \"I love nirva & data science\",\n",
    "     \"created_at\" : datetime.datetime(2013, 4, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"krupa\", \"data_gal\", \"jahanvi\"] },\n",
    "    {\"id\": 3, \n",
    "     \"username\" : \"dnisarg13\", \n",
    "     \"text\" : \"I love nirva & data science\",\n",
    "     \"created_at\" : datetime.datetime(2013, 4, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"krupa\", \"data_gal\", \"jahanvi\"] },\n",
    "    {\"id\": 4, \n",
    "     \"username\" : \"nirvavyas\", \n",
    "     \"text\" : \"i love nisarg\",\n",
    "     \"created_at\" : datetime.datetime(2013, 6, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"data_guy\", \"nisarg\", \"nirva\"] }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (6, 2)]\n"
     ]
    }
   ],
   "source": [
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)\n",
    "        \n",
    "data_science_days = map_reduce(status_updates, \n",
    "                               data_science_day_mapper, \n",
    "                               sum_reducer)\n",
    "print(data_science_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine another task. Let's say we want to profile each user by the most common word they put their status update. There are really three possible approaches. Which is right?\n",
    "1. key is username, values are words and counts\n",
    "2. key is word, values are usernames and counts\n",
    "3. key is username and word, values are counts\n",
    "\n",
    "Let's define a mapper and reducer for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', ('is', 1)), ('dnisarg13', ('i', 2)), ('nirvavyas', ('i', 1))]\n"
     ]
    }
   ],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    user=status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user,(word,1))\n",
    "            \n",
    "def most_popular_word_reducer(user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs, \n",
    "    return the word with the highest total count\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "    \n",
    "    # find most common word and retun that (key,value) pair\n",
    "    word,count = word_counts.most_common(1)[0]\n",
    "    \n",
    "    yield (user,(word,count))\n",
    "    \n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper, \n",
    "                        most_popular_word_reducer)\n",
    "print(user_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', 3), ('dnisarg13', 3), ('nirvavyas', 3)]\n"
     ]
    }
   ],
   "source": [
    "def liker_mapper(status_update):\n",
    "    \"\"\"return (user,liker) pairs\"\"\"\n",
    "    user = status_update[\"username\"]\n",
    "    \n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield (user,liker)\n",
    "distinct_likers_per_user = map_reduce(status_updates,\n",
    "                                     liker_mapper,\n",
    "                                     count_distinct_reducer)\n",
    "print(distinct_likers_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's end this lesson by defining a mapper and reducer for matrix multiplication. Let's assume an $m\\times n$ matrix $A$, and an $n\\times k$ matrix $B$.\n",
    "\n",
    "$C_{ij} = \\sum_{l=1}^n A_{il}B_{lj}$\n",
    "\n",
    "Assume the matrices\n",
    "A = [[3, 2, 0],\n",
    "    [0, 0, 0]]\n",
    "B = [[4, -1, 0],\n",
    "    [10, 0, 0],\n",
    "    [0, 0, 0]]\n",
    "are stored in a common list organized as so:\n",
    "\n",
    "entries = [(\"A\",0,0,3), (\"A\",0,1,2),\n",
    "            (\"B\",0,0,4), (\"B\",0,1,-1), (\"B\",1,0,10)]\n",
    "\n",
    "Our mapper will return the key-value pair ((row,col) of $C$, (col of $A$, value of $A$)) for elements of $A$ and ((row,col) of $C$, (row of $B$, value of $B$)) for elements of $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_mapper(n, element):\n",
    "    \"\"\"n is the common dimension (columns of A, rows of B)\n",
    "    element is a tuple (matrix_name, i, j, value)\"\"\"\n",
    "    matrix, i, j, value = element\n",
    "\n",
    "    # if matrix is A then output the key-value pairs ((i,column), (j,value)) over all columns of C\n",
    "    if matrix == 'A':\n",
    "        for column in range(n):\n",
    "            yield ((i,column),(j,value))\n",
    "    else:\n",
    "        for row in range(n):\n",
    "            yield ((row,j),(i,value))\n",
    "            \n",
    "    \n",
    "    # else if matrix is B then output the key-value pairs ((row, j), (i, value)) over all rows of C\n",
    "     \n",
    "def matrix_multiply_reducer(n, key, indexed_values):\n",
    "    results_by_index = defaultdict(list)\n",
    "    \n",
    "    # this reducer works the same as the word count reducer,\n",
    "    # collecting all the pairs of A and B for each element of C\n",
    "    for index,value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "        \n",
    "    # sum up all the products of the positions with two (non-zero) results\n",
    "    sum_product = sum(results[0]*results[1]\n",
    "                      for results in results_by_index.values()\n",
    "                      if len(results) == 2)\n",
    "    # finally if the terms are != 0 then yield (key, value), where value is the result of the sum-product\n",
    "    if sum_product != 0:\n",
    "        yield (key,sum_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 32), ((0, 1), -3)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = [(\"A\", 0, 0, 3), (\"A\", 0, 1,  2),\n",
    "           (\"B\", 0, 0, 4), (\"B\", 0, 1, -1), (\"B\", 1, 0, 10)]\n",
    "mapper = partial(matrix_multiply_mapper, 3) # what does partial do here?\n",
    "reducer = partial(matrix_multiply_reducer, 3)\n",
    "map_reduce(entries,mapper,reducer) # [((0, 0), 32), ((0, 1), -3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Detailed Solution for Mapper of Mappers in Class Notebook\n",
    "### Part 1. Design the Mappers of Mapper\n",
    "#### Designing the mapper of mapper. I've designed the mapper inside mapper. The inherent method of calling mappers. Mapping can be parallelized so I did design the mapper which can be called and executed within the different parent mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input the File names from User to Map & Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing names of files\n",
    "filenames = [\"data/genesis.txt\",\n",
    "            \"data/Luke.txt\",\n",
    "            \"data/Kings.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designning the Sub Mapper which will get call on the main mappers.\n",
    "#### I've designed the mappers of mappers below which uses the following mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've designed the custom mappers who maps locally withing one main mapper for each files .\n",
    "- Mapping files locally\n",
    "- One Main function to map everything\n",
    "- One only Fn calling require for n number of files\n",
    "- Multitasker mapper function for each and every work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating lists\n",
    "\n",
    "mapper_list1 = []\n",
    "mapper_list2 = []\n",
    "mapper_list3 = []\n",
    "\n",
    "#creating sub mapper 1\n",
    "def sub_mapper1(file):    \n",
    "        with open(f1,'r') as file:\n",
    "            for line in file:\n",
    "                for word in line.strip().split():\n",
    "                    mapper_list1.append((word,1))\n",
    "            return mapper_list1\n",
    "\n",
    "#creating sub mapper 2\n",
    "def sub_mapper2(file):                \n",
    "        with open(f2,'r') as file:\n",
    "            for line in file:\n",
    "                for word in line.strip().split():\n",
    "                    mapper_list2.append((word,1))\n",
    "            return mapper_list2\n",
    "\n",
    "#creating sub mapper 3\n",
    "def sub_mapper3(file):                \n",
    "        with open(f3,'r') as file:\n",
    "            for line in file:\n",
    "                for word in line.strip().split():\n",
    "                    mapper_list3.append((word,1))\n",
    "            return mapper_list3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internal behavior of main mapper below \n",
    "#### Submappers Calling Internally in the main mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1,f2,f3 = filenames\n",
    "- sub_mapper1(f1)\n",
    "- sub_mapper2(f2)\n",
    "- sub_mapper3(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper of Mapper\n",
    "#### Main Mapper function within the mapper. Which uses sub mappers to map different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#desgning the main mapper\n",
    "def mapper0f_mapper(filenames):\n",
    "    f1,f2,f3 = filenames\n",
    "    return sub_mapper1(f1),sub_mapper2(f2),sub_mapper3(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer for Mappers of Mappers'\n",
    "#### Unified reducer to reduce to the N most occuring words in the files\n",
    "- Reducing output of mappers for Sum_Reducer\n",
    "- One fn for every sum_reducer\n",
    "- Calling within simplified MapReduce Procedure\n",
    "- More procedural dependence on given files within simplified single main MapReduce procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement the most popular word reducer\n",
    "def most_popular_word_reducer(mapper_list,n):\n",
    "    \"\"\"given a sequence of (word, count) pairs, \n",
    "    return the word with the highest total count\"\"\"\n",
    "    c = []\n",
    "    word_counts = Counter()\n",
    "    for word, count in mapper_list:\n",
    "        word_counts[word] += count\n",
    "    \n",
    "    # find most common word and retun that (key,value) pair\n",
    "    word,count = word_counts.most_common(n)[n-1]\n",
    "    \n",
    "    c.append((word,count))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top function for getting top buzzwords from the list of documents\n",
    "- Creates Customization for getting desired top N words\n",
    "- Calls within single reducer for n number of files\n",
    "- Optimized for as Time & Space complexity because of task handling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating list to pop n most occuring word elements\n",
    "pop = []\n",
    "\n",
    "def top(mapper_list,n):\n",
    "    for i in range(1,n+1):\n",
    "        mp = most_popular_word_reducer(mapper_list,i)\n",
    "        #list append\n",
    "        pop.append(mp)\n",
    "    \n",
    "    print(\"The top %d words in the document are:\" %n)\n",
    "    \n",
    "    #generating dataframe for better presentation\n",
    "    df = pd.DataFrame(pop,columns=['(Word,Frquency)'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Design Reducer which returns the top n words from list of document files\n",
    "### Document one MapReduce Results\n",
    "#### Displaying top 10 words occuring in the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words in the document are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Word,Frquency)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 2401)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(and, 2354)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(of, 1339)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(And, 1239)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(his, 637)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(he, 634)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(to, 603)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(unto, 588)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(in, 574)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(that, 470)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  (Word,Frquency)\n",
       "0     (the, 2401)\n",
       "1     (and, 2354)\n",
       "2      (of, 1339)\n",
       "3     (And, 1239)\n",
       "4      (his, 637)\n",
       "5       (he, 634)\n",
       "6       (to, 603)\n",
       "7     (unto, 588)\n",
       "8       (in, 574)\n",
       "9     (that, 470)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling top function to MapReduce the given documents.\n",
    "top(mapper_list1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying top 10 words occuring in the second document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words in the document are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Word,Frquency)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 1307)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(and, 1185)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(of, 777)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(And, 694)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(he, 625)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(to, 529)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(that, 443)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(unto, 391)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(in, 342)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(they, 330)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  (Word,Frquency)\n",
       "0     (the, 1307)\n",
       "1     (and, 1185)\n",
       "2       (of, 777)\n",
       "3      (And, 694)\n",
       "4       (he, 625)\n",
       "5       (to, 529)\n",
       "6     (that, 443)\n",
       "7     (unto, 391)\n",
       "8       (in, 342)\n",
       "9     (they, 330)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top(mapper_list2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying top 10 words occuring in the third document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words in the document are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Word,Frquency)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 2115)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(and, 1247)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(of, 1150)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(And, 588)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(to, 460)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(he, 442)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(in, 437)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(that, 323)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(his, 311)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(unto, 279)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  (Word,Frquency)\n",
       "0     (the, 2115)\n",
       "1     (and, 1247)\n",
       "2      (of, 1150)\n",
       "3      (And, 588)\n",
       "4       (to, 460)\n",
       "5       (he, 442)\n",
       "6       (in, 437)\n",
       "7     (that, 323)\n",
       "8      (his, 311)\n",
       "9     (unto, 279)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top(mapper_list3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Tweaks in the same procedure\n",
    "### Automation in given calling procedure for ease of usage\n",
    "- Processes Every Documents in the list\n",
    "- Outputs the dataframes which contains top N buzzwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doclists = [mapper_list1,mapper_list2,mapper_list3]\n",
    "def auto_top(doclists):\n",
    "    for doclist in doclists:\n",
    "        return top(doclist,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Write a mapper and reducer to process all the files available at https://www.ssa.gov/oact/babynames/names.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting files in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting filenames into list\n",
    "#babynamefiles = [\"names/yob1880.txt\",\"names/yob1881.txt\"]\n",
    "babynamefiles = [\"names/yob1880.txt\",\"names/yob1881.txt\",\"names/yob1882.txt\",\"names/yob1883.txt\",\"names/yob1884.txt\",\"names/yob1885.txt\",\"names/yob1886.txt\",\"names/yob1887.txt\",\"names/yob1888.txt\",\"names/yob1889.txt\",\"names/yob1890.txt\",\"names/yob1891.txt\",\"names/yob1892.txt\",\"names/yob1893.txt\",\"names/yob1894.txt\",\"names/yob1895.txt\",\"names/yob1896.txt\",\"names/yob1897.txt\",\"names/yob1898.txt\",\"names/yob1899.txt\",\"names/yob1900.txt\",\"names/yob1901.txt\",\"names/yob1902.txt\",\"names/yob1903.txt\",\"names/yob1904.txt\",\"names/yob1905.txt\",\"names/yob1906.txt\",\"names/yob1907.txt\",\"names/yob1908.txt\",\"names/yob1909.txt\",\"names/yob1910.txt\",\"names/yob1911.txt\",\"names/yob1912.txt\",\"names/yob1913.txt\",\"names/yob1914.txt\",\"names/yob1915.txt\",\"names/yob1916.txt\",\"names/yob1917.txt\",\"names/yob1918.txt\",\"names/yob1919.txt\",\"names/yob1920.txt\",\"names/yob1921.txt\",\"names/yob1922.txt\",\"names/yob1923.txt\",\"names/yob1924.txt\",\"names/yob1925.txt\",\"names/yob1926.txt\",\"names/yob1927.txt\",\"names/yob1928.txt\",\"names/yob1929.txt\",\"names/yob1930.txt\",\"names/yob1931.txt\",\"names/yob1932.txt\",\"names/yob1933.txt\",\"names/yob1934.txt\",\"names/yob1935.txt\",\"names/yob1936.txt\",\"names/yob1937.txt\",\"names/yob1938.txt\",\"names/yob1939.txt\",\"names/yob1940.txt\",\"names/yob1941.txt\",\"names/yob1942.txt\",\"names/yob1943.txt\",\"names/yob1944.txt\",\"names/yob1945.txt\",\"names/yob1946.txt\",\"names/yob1947.txt\",\"names/yob1948.txt\",\"names/yob1949.txt\",\"names/yob1950.txt\",\"names/yob1951.txt\",\"names/yob1952.txt\",\"names/yob1953.txt\",\"names/yob1954.txt\",\"names/yob1955.txt\",\"names/yob1956.txt\",\"names/yob1957.txt\",\"names/yob1958.txt\",\"names/yob1959.txt\",\"names/yob1960.txt\",\"names/yob1961.txt\",\"names/yob1962.txt\",\"names/yob1963.txt\",\"names/yob1964.txt\",\"names/yob1965.txt\",\"names/yob1966.txt\",\"names/yob1967.txt\",\"names/yob1968.txt\",\"names/yob1969.txt\",\"names/yob1970.txt\",\"names/yob1971.txt\",\"names/yob1972.txt\",\"names/yob1973.txt\",\"names/yob1974.txt\",\"names/yob1975.txt\",\"names/yob1976.txt\",\"names/yob1977.txt\",\"names/yob1978.txt\",\"names/yob1979.txt\",\"names/yob1980.txt\",\"names/yob1981.txt\",\"names/yob1982.txt\",\"names/yob1983.txt\",\"names/yob1984.txt\",\"names/yob1985.txt\",\"names/yob1986.txt\",\"names/yob1987.txt\",\"names/yob1988.txt\",\"names/yob1989.txt\",\"names/yob1990.txt\",\"names/yob1991.txt\",\"names/yob1992.txt\",\"names/yob1993.txt\",\"names/yob1994.txt\",\"names/yob1995.txt\",\"names/yob1996.txt\",\"names/yob1997.txt\",\"names/yob1998.txt\",\"names/yob1999.txt\",\"names/yob2000.txt\",\"names/yob2001.txt\",\"names/yob2002.txt\",\"names/yob2003.txt\",\"names/yob2004.txt\",\"names/yob2005.txt\",\"names/yob2006.txt\",\"names/yob2007.txt\",\"names/yob2008.txt\",\"names/yob2009.txt\",\"names/yob2010.txt\",\"names/yob2011.txt\",\"names/yob2012.txt\",\"names/yob2013.txt\",\"names/yob2014.txt\",\"names/yob2015.txt\",\"names/yob2016.txt\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper of Files\n",
    "#### I'm designing the mapper for all files which maps data from every file into (NAME,FREQUENCY) pair.\n",
    "- Input the list of various files\n",
    "- Gives output bu mapping those file parallelly in different namelists\n",
    "- Input mapperlist to reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a empty list\n",
    "namelist = []\n",
    "\n",
    "#mapper fn\n",
    "def mapper(namefiles):\n",
    "    for namefile in namefiles:\n",
    "        #for all files, open each file\n",
    "        with open(namefile,'r') as file:\n",
    "            for line in file:\n",
    "                for word in line.strip().split(','):\n",
    "                    #append word into list\n",
    "                    namelist.append((word))\n",
    "    #return final list\n",
    "    return namelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The List values removal function which is supporting function of reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for removing values\n",
    "def remove_values_from_list(the_list, val):\n",
    "        while val in the_list:\n",
    "            the_list.remove(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reducer for finding & grouping the name data with correspoding frequency of occurence\n",
    "#### The Reducer returns,\n",
    "####     -  Baby names starting from perticular letter\n",
    "####     -  Occurences of those names\n",
    "####     -  Top N values starting with perticular character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reducer fn\n",
    "def reducer(namelist,startingwith,topnwords):\n",
    "    #organizing values\n",
    "    remove_values_from_list(namelist,\"F\" and \"M\")\n",
    "    \n",
    "    #extracting values\n",
    "    list_names = namelist[0::2]\n",
    "    list_count = namelist[1::2]\n",
    "    \n",
    "    #creating dataframe for clean representation\n",
    "    babyname_list = pd.DataFrame(\n",
    "    {'Names': list_names,\n",
    "     'Names total Count': list_count\n",
    "    })\n",
    "    \n",
    "    #extracting required rows\n",
    "    babyname_list['Starting with?'] = babyname_list['Names'].str.extract('(^%s*)'%startingwith, expand=False).str.strip()\n",
    "    babyname_list.sort_values(by='Names total Count', ascending=0)\n",
    "    result = babyname_list.loc[babyname_list['Starting with?'] == \"M\"]\n",
    "    \n",
    "    return result.head(topnwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Routine which incorporates my custom designed Mapper + Reducer\n",
    "- Unified modelling of mapper+reducer in singular function\n",
    "- Contains Mappers & Reducer functionality for list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapReduce(namefiles,namelist,startingwith,topnwords):\n",
    "    #Calling Mapper\n",
    "    mapper(namefiles)\n",
    "    #returning result from reducer\n",
    "    return reducer(namelist,startingwith,topnwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling MapReduce Routine for Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Names total Count</th>\n",
       "      <th>Starting with?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Minnie</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Maude</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mattie</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Marie</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>May</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Mae</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Mollie</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Matilda</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Mildred</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Names Names total Count Starting with?\n",
       "0       Mary                 F              M\n",
       "6     Minnie                 F              M\n",
       "30     Maude                 F              M\n",
       "42    Mattie                 F              M\n",
       "81     Marie                 F              M\n",
       "84       May                 F              M\n",
       "105      Mae                 F              M\n",
       "120   Mollie                 F              M\n",
       "150  Matilda                 F              M\n",
       "180  Mildred                 F              M"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing filename with starting letter that we wanna extract & no of top results.\n",
    "MapReduce(babynamefiles,namelist,\"M\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Writing Mapper and Reducer that returns the top 'n' names from a list of files that contain (anywhere within the name) a given string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting filenames into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#babynamefiles = [\"names/yob1880.txt\",\"names/yob1881.txt\"]\n",
    "babynamefiles = [\"names/yob1880.txt\",\"names/yob1881.txt\",\"names/yob1882.txt\",\"names/yob1883.txt\",\"names/yob1884.txt\",\"names/yob1885.txt\",\"names/yob1886.txt\",\"names/yob1887.txt\",\"names/yob1888.txt\",\"names/yob1889.txt\",\"names/yob1890.txt\",\"names/yob1891.txt\",\"names/yob1892.txt\",\"names/yob1893.txt\",\"names/yob1894.txt\",\"names/yob1895.txt\",\"names/yob1896.txt\",\"names/yob1897.txt\",\"names/yob1898.txt\",\"names/yob1899.txt\",\"names/yob1900.txt\",\"names/yob1901.txt\",\"names/yob1902.txt\",\"names/yob1903.txt\",\"names/yob1904.txt\",\"names/yob1905.txt\",\"names/yob1906.txt\",\"names/yob1907.txt\",\"names/yob1908.txt\",\"names/yob1909.txt\",\"names/yob1910.txt\",\"names/yob1911.txt\",\"names/yob1912.txt\",\"names/yob1913.txt\",\"names/yob1914.txt\",\"names/yob1915.txt\",\"names/yob1916.txt\",\"names/yob1917.txt\",\"names/yob1918.txt\",\"names/yob1919.txt\",\"names/yob1920.txt\",\"names/yob1921.txt\",\"names/yob1922.txt\",\"names/yob1923.txt\",\"names/yob1924.txt\",\"names/yob1925.txt\",\"names/yob1926.txt\",\"names/yob1927.txt\",\"names/yob1928.txt\",\"names/yob1929.txt\",\"names/yob1930.txt\",\"names/yob1931.txt\",\"names/yob1932.txt\",\"names/yob1933.txt\",\"names/yob1934.txt\",\"names/yob1935.txt\",\"names/yob1936.txt\",\"names/yob1937.txt\",\"names/yob1938.txt\",\"names/yob1939.txt\",\"names/yob1940.txt\",\"names/yob1941.txt\",\"names/yob1942.txt\",\"names/yob1943.txt\",\"names/yob1944.txt\",\"names/yob1945.txt\",\"names/yob1946.txt\",\"names/yob1947.txt\",\"names/yob1948.txt\",\"names/yob1949.txt\",\"names/yob1950.txt\",\"names/yob1951.txt\",\"names/yob1952.txt\",\"names/yob1953.txt\",\"names/yob1954.txt\",\"names/yob1955.txt\",\"names/yob1956.txt\",\"names/yob1957.txt\",\"names/yob1958.txt\",\"names/yob1959.txt\",\"names/yob1960.txt\",\"names/yob1961.txt\",\"names/yob1962.txt\",\"names/yob1963.txt\",\"names/yob1964.txt\",\"names/yob1965.txt\",\"names/yob1966.txt\",\"names/yob1967.txt\",\"names/yob1968.txt\",\"names/yob1969.txt\",\"names/yob1970.txt\",\"names/yob1971.txt\",\"names/yob1972.txt\",\"names/yob1973.txt\",\"names/yob1974.txt\",\"names/yob1975.txt\",\"names/yob1976.txt\",\"names/yob1977.txt\",\"names/yob1978.txt\",\"names/yob1979.txt\",\"names/yob1980.txt\",\"names/yob1981.txt\",\"names/yob1982.txt\",\"names/yob1983.txt\",\"names/yob1984.txt\",\"names/yob1985.txt\",\"names/yob1986.txt\",\"names/yob1987.txt\",\"names/yob1988.txt\",\"names/yob1989.txt\",\"names/yob1990.txt\",\"names/yob1991.txt\",\"names/yob1992.txt\",\"names/yob1993.txt\",\"names/yob1994.txt\",\"names/yob1995.txt\",\"names/yob1996.txt\",\"names/yob1997.txt\",\"names/yob1998.txt\",\"names/yob1999.txt\",\"names/yob2000.txt\",\"names/yob2001.txt\",\"names/yob2002.txt\",\"names/yob2003.txt\",\"names/yob2004.txt\",\"names/yob2005.txt\",\"names/yob2006.txt\",\"names/yob2007.txt\",\"names/yob2008.txt\",\"names/yob2009.txt\",\"names/yob2010.txt\",\"names/yob2011.txt\",\"names/yob2012.txt\",\"names/yob2013.txt\",\"names/yob2014.txt\",\"names/yob2015.txt\",\"names/yob2016.txt\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Mapper\n",
    "#### I'm designing the mapper for all files which maps data from every file into (NAME,FREQUENCY) pair.\n",
    "- Input the list of various files\n",
    "- Gives output bu mapping those file parallelly in different namelists\n",
    "- Input mapperlist to reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namelist = []\n",
    "def mapper(namefiles):\n",
    "    for namefile in namefiles:\n",
    "        #opening file\n",
    "        with open(namefile,'r') as file:\n",
    "            for line in file:\n",
    "                for word in line.strip().split(','):\n",
    "                    #appending pair into list\n",
    "                    namelist.append((word))\n",
    "    return namelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reducer for finding & grouping the name data with correspoding frequency of occurence\n",
    "#### The Reducer returns,\n",
    "####     -  Baby names which contains letters from perticular word record\n",
    "####     -  Occurences of those names\n",
    "####     -  Top N values which contains those perticular characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer(namelist,contains,topnwords):\n",
    "    remove_values_from_list(namelist,\"F\" and \"M\")\n",
    "    \n",
    "    #organizing list\n",
    "    list_names = namelist[0::2]\n",
    "    list_count = namelist[1::2]\n",
    "    \n",
    "    #creating dataframe for better representation\n",
    "    babyname_list = pd.DataFrame(\n",
    "    {'Names': list_names,\n",
    "     'Names total Count': list_count\n",
    "    })\n",
    "    \n",
    "    #extracting required rows from datafame\n",
    "    babyname_list['Contains?'] = babyname_list['Names'].str.extract('(%s)'%contains, expand=False).str.strip()\n",
    "    babyname_list.sort_values(by='Names total Count', ascending=0)\n",
    "    result = babyname_list.loc[babyname_list['Contains?'] == \"ar\"]\n",
    "    \n",
    "    #return the results list\n",
    "    return result.head(topnwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Routine which incorporates my custom designed Mapper + Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapReduce(namefiles,namelist,contains,topnwords):\n",
    "    mapper(namefiles)\n",
    "    return reducer(namelist,contains,topnwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling MapReduce Routine for Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Names total Count</th>\n",
       "      <th>Contains?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Pearl</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Marie</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Harriet</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Caroline</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Maria</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Marion</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Carolyn</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Marian</td>\n",
       "      <td>F</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Names Names total Count Contains?\n",
       "0         Mary                 F        ar\n",
       "69       Pearl                 F        ar\n",
       "81       Marie                 F        ar\n",
       "108    Harriet                 F        ar\n",
       "111   Caroline                 F        ar\n",
       "135  Charlotte                 F        ar\n",
       "195      Maria                 F        ar\n",
       "204     Marion                 F        ar\n",
       "243    Carolyn                 F        ar\n",
       "306     Marian                 F        ar"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fn call for final output\n",
    "MapReduce(babynamefiles,namelist,\"ar\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
